{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'llm' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n llm ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# ! pip install langchain tiktoken pypdf chromadb\n",
    "# ! pip install transformers sentence_transformers==2.2.2\n",
    "# ! pip install accelerate \n",
    "# ! pip install plum-dispatch==1.7.4 aspose.words Spire.PDF\n",
    "# # bitsandbytes xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading necessary modeules\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import  HuggingFaceBgeEmbeddings\n",
    "import aspose.words as aw\n",
    "from spire.pdf.common import *\n",
    "from spire.pdf import *\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Pdf\n",
    "dir_name = \"bw_pdf\"\n",
    "file_loader = DirectoryLoader(dir_name, glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = file_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing New line Characters\n",
    "for doc in documents:\n",
    "    doc.page_content = doc.page_content.replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intializing TextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting Documents to chunks\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Extract Table from document and return prompt converted to document\n",
    "from langchain.docstore.document import Document\n",
    "def extract_dataframe_list(file_path, temp_dir):\n",
    "    page = []\n",
    "    try:\n",
    "        file_extention = file_path[file_path.rindex('.'):]\n",
    "        df_list = []\n",
    "        if (file_extention in [\".xlsx\", \".csv\", \".html\", \".pdf\"]):\n",
    "            if (file_extention == \".html\"):\n",
    "                df_list = pd.read_html(file_path)\n",
    "            elif (file_extention == \".csv\"):\n",
    "                df_list.append(pd.read_csv(file_path))\n",
    "            elif (file_extention == \".xlsx\"):\n",
    "                df_list.append(pd.read_excel(file_path))\n",
    "            elif (file_extention == \".pdf\"):\n",
    "                pdf = PdfDocument()\n",
    "                pdf.LoadFromFile(file_path)\n",
    "                total_pages = pdf.Pages.Count\n",
    "                pdf.Split('/kaggle/working/temp_file_{0}.pdf')\n",
    "                for i in range(total_pages):\n",
    "                    temp_html_name = f'/kaggle/working/temp_file_{i}.html'\n",
    "                    temp_pdf_name = f'/kaggle/working/temp_file_{i}.pdf'\n",
    "\n",
    "                    doc = aw.Document(temp_pdf_name)\n",
    "                    os.remove(temp_pdf_name)\n",
    "                    doc.save(temp_html_name)\n",
    "\n",
    "                    try:\n",
    "                        ls = pd.read_html(temp_html_name)\n",
    "                        os.remove(temp_html_name)\n",
    "                        df_list.extend(ls)\n",
    "                        for j in range(len(ls)):\n",
    "                            page.append(i+1)\n",
    "                    except:\n",
    "                        print('-', end='')\n",
    "                for filename in os.listdir(temp_dir):\n",
    "                    if os.path.isfile(os.path.join(temp_dir, filename)):\n",
    "                        os.remove(os.path.join(temp_dir, filename))\n",
    "\n",
    "        return df_list, page\n",
    "    except:\n",
    "        return [], page\n",
    "\n",
    "\n",
    "def is_incremental(s1, s2):\n",
    "    s1_ascii, s2_ascii = sum([ord(ele)\n",
    "                             for ele in s1]), sum([ord(ele) for ele in s2])\n",
    "    if (s1_ascii+1 == s2_ascii or s1_ascii == s2_ascii):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def check_incremental_row(tab_1, tab_2):\n",
    "    # If first column incremental\n",
    "    tab_1_last = tab_1.iloc[-1, 0]\n",
    "    tab_2_first = tab_2.iloc[0, 0]\n",
    "    tab_2_1st_col_name = tab_2.columns[0]\n",
    "\n",
    "    # IF TAB1(1,LAST) == TAB2(1,1)\n",
    "    if (is_incremental(str(tab_1_last), str(tab_2_first))):\n",
    "        tab_1.loc[len(tab_1)] = tab_2.values[0]\n",
    "        tab_2.columns = tab_1.columns\n",
    "        return True\n",
    "    # IF TAB1(1,LAST) == COL_NAME(1)\n",
    "    elif (is_incremental(str(tab_1_last), str(tab_2_1st_col_name))):\n",
    "        tab_2.columns\n",
    "        tab_1.loc[len(tab_1)] = tab_2.columns\n",
    "        tab_2.columns = tab_1.columns\n",
    "\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def merge_same_column(df):\n",
    "    i = 1\n",
    "    while i < len(df.columns):\n",
    "        if (df.columns[i-1] == df.columns[i]):\n",
    "            for j in df.iterrows():\n",
    "                if (j[1][0]) != j[1][1]:\n",
    "                    val = f\"{j[1][0]}\"+f\"{j[1][1]}\"\n",
    "                    j[1][0] = j[1][1] = val\n",
    "\n",
    "            df = df.loc[:, ~df.columns.duplicated()].copy()\n",
    "\n",
    "        else:\n",
    "\n",
    "            same_in_both_column = 0\n",
    "            for j in df.iterrows():\n",
    "                try:\n",
    "                    if (j[1, 0]) == j[1, 1]:\n",
    "                        same_in_both_column += 1\n",
    "                except:\n",
    "                    continue\n",
    "            if (same_in_both_column == df.shape[0]):\n",
    "                val = f\"{df.columns[i-1]}\"+f\"{df.columns[i]}\"\n",
    "                temp_dict = {df.columns[i-1]: val, df.columns[i]: val}\n",
    "                df.rename(columns=temp_dict, inplace=True)\n",
    "            else:\n",
    "                i += 1\n",
    "            df = df.loc[:, ~df.columns.duplicated()].copy()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_table(df_list, page):\n",
    "    # filter individual table\n",
    "    filtered_pages = []\n",
    "    clean_tables = []\n",
    "    for i, df in enumerate(df_list):\n",
    "\n",
    "        #         df = df.fillna(\"\")\n",
    "        merge_same_column(df)\n",
    "\n",
    "        total_row = df.shape[0]\n",
    "        total_column = df.shape[1]\n",
    "\n",
    "        # to specify the column name if named [0,1,2,3...]\n",
    "        if (df.columns == range(total_column)).all():\n",
    "            updated_column_names = dict(zip(df.columns, df.iloc[0]))\n",
    "\n",
    "            # Drop the first row since it's used for column renaming\n",
    "            df = df.drop(0)\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "            total_row -= 1\n",
    "\n",
    "            # Rename columns using the updated_column_names dictionary\n",
    "            df.rename(columns=updated_column_names, inplace=True)\n",
    "\n",
    "       # Delete rows if all elements are the same\n",
    "        all_same_rows = df.apply(lambda row: row.nunique() != 1, axis=1)\n",
    "        df = df[all_same_rows]\n",
    "\n",
    "        # Delete columns if column name is null or if all elements are null\n",
    "        df = df.dropna(axis=1, how='all')\n",
    "\n",
    "        # delete columns if column name is nan\n",
    "        df = df.drop(columns=[col for col in df.columns if col is np.nan])\n",
    "\n",
    "        df = df.fillna(\"\")\n",
    "        df.index = range(len(df.index))\n",
    "        try:\n",
    "            if ((df.columns == [df.columns[0]]*len(df.columns)).all()):\n",
    "                df.columns = df.iloc[0]\n",
    "                df.drop(df.index[0], inplace=True)\n",
    "                df.reset_index(drop=True, inplace=True)\n",
    "            else:\n",
    "                df = merge_same_column(df)\n",
    "        except:\n",
    "            print('error')\n",
    "        filtered_pages.append(page[i])\n",
    "        clean_tables.append(df)\n",
    "\n",
    "    # merge tables\n",
    "    i = 1\n",
    "    while i < len(clean_tables):\n",
    "        try:\n",
    "\n",
    "            if clean_tables[i].shape[1] == clean_tables[i-1].shape[1]:\n",
    "                # IF NO OF COLUMN SAME\n",
    "                if ((clean_tables[i].columns == clean_tables[i-1].columns).all()):\n",
    "\n",
    "                    # JOIN IF COLUMN NAME SAME\n",
    "                    clean_tables[i -\n",
    "                                 1] = pd.concat([clean_tables[i-1], clean_tables[i]])\n",
    "                    clean_tables.pop(i)\n",
    "                    filtered_pages.remove(filtered_pages[i])\n",
    "\n",
    "                elif (check_incremental_row(clean_tables[i-1], clean_tables[i])):\n",
    "                    clean_tables[i -\n",
    "                                 1] = pd.concat([clean_tables[i-1], clean_tables[i]])\n",
    "                    clean_tables.pop(i)\n",
    "                    filtered_pages.remove(filtered_pages[i])\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "        except:\n",
    "            i += 1\n",
    "#     fcl = []\n",
    "#     for df in clean_tables:\n",
    "#         # delete single row column table\n",
    "# #         if (df.shape[0] <= 1 or df.shape[1] <=1):\n",
    "# #             continue\n",
    "# # #         elif()\n",
    "# #         else:\n",
    "#         fcl.append(df)\n",
    "\n",
    "    return clean_tables, filtered_pages\n",
    "\n",
    "\n",
    "def give_prompt_from_document(file_path):\n",
    "    extracted_dataframes, pages = extract_dataframe_list(\n",
    "        file_path, '/kaggle/working/')\n",
    "\n",
    "    filtered_tables, filtered_pages = filter_table(extracted_dataframes, pages)\n",
    "#     print(len(filtered_tables), len(filtered_pages))\n",
    "\n",
    "    doc_list = []\n",
    "    for x, df in enumerate(filtered_tables):\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        des_list = []\n",
    "        for i in range(df.shape[0]):\n",
    "            description = f\"For {i} \"\n",
    "            row_values = [\n",
    "                f\"{df.columns[j]} is {df.iloc[i,j]}\" for j in range(df.shape[1])]\n",
    "            null_element_in_row = sum(1 for value in row_values if value == \"\")\n",
    "            description += \", \".join(row_values) + \".\"\n",
    "            des_list.append(description)\n",
    "        doc_text = ''\n",
    "        for i, des in enumerate(des_list):\n",
    "            if len(doc_text+des) < 1200:\n",
    "                doc_text += des\n",
    "                if i == len(des_list)-1:\n",
    "                    doc = Document(page_content=doc_text, metadata={\n",
    "                                   \"source\": file_path, \"page\": filtered_pages[x]})\n",
    "                    doc_list.append(doc)\n",
    "            else:\n",
    "                doc = Document(page_content=doc_text, metadata={\n",
    "                               \"source\": file_path, \"page\": filtered_pages[x]})\n",
    "                doc_list.append(doc)\n",
    "                doc_text = des\n",
    "\n",
    "#         prompt = \"\\n\\n\".join(des_list)\n",
    "#         doc =  Document(page_content=prompt, metadata={\"source\": file_path, \"page\": filtered_pages[x]})\n",
    "#         doc_list.append(doc)\n",
    "#         final_prompt += \"\\n\\n\" + prompt\n",
    "\n",
    "    return doc_list, filtered_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/19 [00:46<13:53, 46.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2/19 [00:58<07:29, 26.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 3/19 [03:01<18:44, 70.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 4/19 [06:10<29:16, 117.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 5/19 [08:22<28:38, 122.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 6/19 [08:31<18:10, 83.91s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 7/19 [09:24<14:45, 73.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 8/19 [09:29<09:30, 51.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 9/19 [09:57<07:23, 44.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 10/19 [09:57<04:37, 30.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 11/19 [10:09<03:20, 25.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 12/19 [10:14<02:11, 18.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "doc_list = []\n",
    "for filename in tqdm(os.listdir(dir_name)):\n",
    "    if os.path.isfile(os.path.join(dir_name, filename)):\n",
    "        print(os.path.join(dir_name, filename))\n",
    "        doc_list.extend(give_prompt_from_document(\n",
    "            os.path.join(dir_name, filename))[0])\n",
    "texts.extend(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c3c7e612b845f294cff6640d22c52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c492c86a680492e906f47c1e740b2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a361966f5f664b52960c0fcb00f5d6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f349c03cd94ad0a6ee9c3fdaa94503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c4eee0445c4bde91c437bf6a1a04a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09cce66a785c4cca8e1d2065ee58929c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "# set True to compute cosine similarity\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
